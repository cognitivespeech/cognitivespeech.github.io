<!DOCTYPE HTML>
<html>
<head>
<meta charset="UTF-8">
<style>
table {
      border-collapse: collapse;
      width: 80%;
}

th {
      padding: 12px;
      text-align: left;
      border-top: 3px solid #ddd;
      border-bottom: 3px solid #ddd;
      font-size: 18px;
}
td {
      padding: 12px;
      text-align: left;
      border-bottom: 3px solid #ddd;
}
</style>
</head>
</head>

<body>
<h1>Boosting diffusion model for spectrum upsampling in text-to-speech: An empirical study</h1>
<p>arXiv: <a href="https://arxiv.org/pdf/2207.04646.pdf">arXiv:2207.04646</a></p>

<h2>Authors</h2>
<ul>
<li style="font-size:20px"> Chong Zhang (Microsoft Azure Speech) </li>
<li style="font-size:20px"> Yanqing Liu (Microsoft Azure Speech) </li>
<li style="font-size:20px"> Yang Zheng  (Microsoft Azure Speech) </li>
<li style="font-size:20px"> Sheng Zhao  (Microsoft Azure Speech) </li>
</ul>


<h2>Abstract</h2>
<p style="font-size:20px">Scaling text-to-speech (TTS) with language model to large-scale and in-the-wild datasets is making great progress to capture the diversity and expressiveness in human speech such as speaker identities and prosodies, but the waveform reconstruction quality from discrete speech token is far from satisfaction mainly depending on compressed speech token size. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech, especially in domains like conversation etc. Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens autoregressively, and then use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms, which has high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of language model and diffusion based architecture, and present the model architecture, objective and subjective measurement difference on the same dataset.
</p>
<h3> </h3>


<h3> </h3>

<h2>Audio Samples</h2>
<table><tr>
<th>Model</th>
<th>step 1</th>
<th>step 6</th>
<th>step 12</th>
</tr>

<tr>
<th>DDPM</th>
<td><audio src="./audio/diffusion/ddpm/1/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/ddpm/6/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/ddpm/12/1.wav" controls="controls">Audio Tag not supported.</audio></td>
</tr>

<tr>
<th>EDM</th>
<td><audio src="./audio/diffusion/edm/1/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/edm/6/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/edm/12/1.wav" controls="controls">Audio Tag not supported.</audio></td>
</tr>

<tr>
<th>flow matching</th>
<td><audio src="./audio/diffusion/flow_matching/1/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/flow_matching/6/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/flow_matching/12/1.wav" controls="controls">Audio Tag not supported.</audio></td>
</tr>


<tr>
<th>reflow</th>
<td><audio src="./audio/diffusion/reflow/1/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/reflow/6/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/reflow/12/1.wav" controls="controls">Audio Tag not supported.</audio></td>
</tr>

<tr>
<th>multiflow</th>
<td><audio src="./audio/diffusion/multiflow/1/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/multiflow/6/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/multiflow/12/1.wav" controls="controls">Audio Tag not supported.</audio></td>
</tr>

<tr>
<th>consistency distill</th>
<td><audio src="./audio/diffusion/cd/1/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/cd/6/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/cd/12/1.wav" controls="controls">Audio Tag not supported.</audio></td>
</tr>

<tr>
<th>bespoke-step5</th>
<td><audio src="./audio/diffusion/bespoke-5/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/bespoke-5/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/bespoke-5/1.wav" controls="controls">Audio Tag not supported.</audio></td>
</tr>

<tr>
<th>bespoke-step8</th>
<td><audio src="./audio/diffusion/bespoke-8/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/bespoke-8/1.wav" controls="controls">Audio Tag not supported.</audio></td>
<td><audio src="./audio/diffusion/bespoke-8/1.wav" controls="controls">Audio Tag not supported.</audio></td>
</tr>

</table>




<br>
<br>
<a style="font-size:20px" href="mailto:yanqliu@microsoft.com">Contact the author</a>
<br>
<br>
<br>
<br>
</body>
</html>
